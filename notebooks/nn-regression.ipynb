{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training a deep neural network for regression using TensorFlow\n",
    "\n",
    "In this notebook a deep neural network is implemented using [TensorFlow](https://www.tensorflow.org/). The network is used to train an imitation learning agent, following the [CS 294 course](http://rll.berkeley.edu/deeprlcourse/). The learning agent's task is to imitate expert's policy.\n",
    "\n",
    "The structure of the network is the following:\n",
    "\n",
    "    INPUT -> FC -> ReLU -> DROPOUT -> FC -> ReLU -> OUTPUT -> L2 LOSS.\n",
    "    \n",
    "Apart from the DNN implementation, this notebook demonstrates methods for loading saved models and accessing desired tensors.\n",
    "\n",
    "We start with the implementation of the network.\n",
    "\n",
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Used to clear up the workspace.\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the data.\n",
    "data = pickle.load(open('../data/data-ant.pkl', 'rb'))\n",
    "actions = data['actions']\n",
    "observations = data['observations']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(observations, actions, test_size=0.20, random_state=42)\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters.\n",
    "model_path = \"./model\"\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "reg = 0.001\n",
    "dropout_prob = 1.0 # No dropout will be performed\n",
    "\n",
    "# Network parameters.\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 128\n",
    "num_inputs = observations.shape[1]\n",
    "num_outputs = actions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tensors.\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_outputs], name='y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# Define weights biases.\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_inputs,   num_hidden_1], stddev=np.sqrt(2.0 / num_inputs))),\n",
    "    'W2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], stddev=np.sqrt(2.0 / num_hidden_1))),\n",
    "    'W3': tf.Variable(tf.random_normal([num_hidden_2, num_outputs],  stddev=np.sqrt(2.0 / num_hidden_2)))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([num_outputs]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "def two_layer_nn(X, weights, biases):\n",
    "    # Hidden layer with ReLU activations.\n",
    "    layer_1 = tf.add(tf.matmul(X, weights['W1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Add dropout.\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n",
    "    # Hidden layer with ReLU activations.\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['W2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation.\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['W3']), biases['b3'], name='pred')\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Construct the model.\n",
    "pred = two_layer_nn(X, weights, biases)\n",
    "\n",
    "# Define the loss and the optimizer.\n",
    "data_cost = tf.reduce_mean(tf.reduce_sum(tf.square(y - pred), axis=1)) / 2\n",
    "reg_cost = reg * (tf.nn.l2_loss(weights['W1']) + tf.nn.l2_loss(weights['W2']) + tf.nn.l2_loss(weights['W3']))\n",
    "cost = tf.add(data_cost, reg_cost, name='cost')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()  # Used to save the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1, cost=0.749728328\n",
      "Epoch:   2, cost=0.178417585\n",
      "Epoch:   3, cost=0.142941582\n",
      "Epoch:   4, cost=0.120632086\n",
      "Epoch:   5, cost=0.104228901\n",
      "Epoch:   6, cost=0.091986890\n",
      "Epoch:   7, cost=0.082274539\n",
      "Epoch:   8, cost=0.074395996\n",
      "Epoch:   9, cost=0.068333641\n",
      "Epoch:  10, cost=0.063371284\n",
      "Optimization finished!\n",
      "\n",
      "Training cost=0.060372, test cost=0.060427\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches.\n",
    "        total_batch = num_train / batch_size\n",
    "        for i in range(total_batch):\n",
    "            curr_id, curr_batch = i * batch_size, batch_size\n",
    "            if i == total_batch - 1:\n",
    "                curr_batch += num_train % batch_size\n",
    "            \n",
    "            batch_x, batch_y = X_train[curr_id : curr_id + curr_batch], y_train[curr_id : curr_id + curr_batch]\n",
    "\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_x, y: batch_y, keep_prob: dropout_prob})\n",
    "            avg_cost += c * curr_batch / num_train\n",
    "        \n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:%4d,\" % (epoch+1), \"cost={:.9f}\".format(avg_cost)\n",
    "    \n",
    "    print \"Optimization finished!\\n\"\n",
    "    \n",
    "    # Save the model.\n",
    "    save_path = saver.save(sess, model_path)\n",
    "\n",
    "    # Test the model.\n",
    "    training_cost = sess.run(cost, feed_dict={X: X_train, y: y_train, keep_prob: 1.0})\n",
    "    test_cost = sess.run(cost, feed_dict={X: X_test, y: y_test, keep_prob: 1.0})\n",
    "    print \"Training cost=%f, test cost=%f\" % (training_cost, test_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model is stored in the *model_filename*, which is, in our case, in the same directory under the name **model**. \n",
    "\n",
    "Now we would like to load the model from a different program and use it to form our predictions. \n",
    "\n",
    "In order to make it easier, it is advised to label the tensors we would like to use. In our use case, we simply want to validate the model or make predictions, so we only need:\n",
    "- **cost** - in order to easily validate the model,\n",
    "- **pred** - in order to make predictions.\n",
    "\n",
    "But, as we will soon see, we also need the input tensors:\n",
    "- **X**, **y**, and **keep_prob**, as they act as placeholders for our desired inputs/outputs.\n",
    "\n",
    "Looking at the code above, we can observe that the variables are named in a sensible way. We will now proceed to using the trained model.\n",
    "\n",
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data.\n",
    "data = pickle.load(open('../data/data-ant.pkl', 'rb'))\n",
    "num_data = data['actions'].shape[0]\n",
    "\n",
    "# Parameters.\n",
    "model_path = \"./model\"\n",
    "\n",
    "# Take a random sample for validation/prediction.\n",
    "num_test = 2\n",
    "idx = np.random.randint(num_data, size=num_test)\n",
    "X = data['observations'][idx]\n",
    "y = data['actions'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Test cost=0.066741\n",
      "[[-0.3160854  -0.01247683  0.44229236 -0.24586061 -0.16247268  0.25423124\n",
      "  -0.44148654  0.14382587]\n",
      " [-0.27930036 -0.22507024  0.40635556  0.01655081  0.1362969   0.07348641\n",
      "  -0.49082947  0.27687025]]\n",
      "[[-0.15878627 -0.02706146  0.30654174 -0.22852671 -0.20345624  0.26499885\n",
      "  -0.43494898  0.1705665 ]\n",
      " [-0.13296083 -0.10233426  0.33795387  0.02566433  0.04228948  0.08005988\n",
      "  -0.52945173  0.30881366]]\n"
     ]
    }
   ],
   "source": [
    "# Load the previous graph.\n",
    "loader = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "\n",
    "# Handle of the loaded graph.\n",
    "g = tf.get_default_graph()\n",
    "\n",
    "cost = g.get_tensor_by_name(\"cost:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loader.restore(sess, model_path)  # Restores all the calculated variables, e.g. weights.\n",
    "    \n",
    "    test_cost = sess.run(cost, feed_dict={'X:0': X, 'y:0': y, 'keep_prob:0': 1.0})\n",
    "    print \"Test cost=%f\" % test_cost\n",
    "    \n",
    "    y_ = sess.run('pred:0', feed_dict={'X:0': X, 'keep_prob:0': 1.0})\n",
    "    \n",
    "    print y\n",
    "    print y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice the lines above:\n",
    "\n",
    "    1. cost = g.get_tensor_by_name(\"cost:0\")\n",
    "    2. sess.run(cost, feed_dict={'X:0': X, 'y:0': y, 'keep_prob:0': 1.0})\n",
    "    3. sess.run('pred:0', feed_dict={'X:0': X, 'keep_prob:0': 1.0})\n",
    "    \n",
    "In practice, the first two lines achieve the same thing as the third line. \n",
    "\n",
    "To access a tensor from the previous session, we can either make a reference to it, if we need to perform future manipulations, or we can pass it by name in case we simply want to evaluate an expression.\n",
    "\n",
    "\n",
    "There is another useful way to do this - by using collections. To do this, we first add the variables we are interested in to collections. We do this during model construction. Later on, we can access the variables from collections. Here is a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def example_code():\n",
    "    # While building the model.\n",
    "    tf.add_to_collection('reuse', cost)\n",
    "    tf.add_to_collection('reuse', pred)\n",
    "\n",
    "    # To retreive the tensor.\n",
    "    pred = tf.get_collection('cost')[1]\n",
    "\n",
    "    # To print the contents of a collection.\n",
    "    print tf.get_collection('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Access unknown tensors\n",
    "\n",
    "So far we have looked into retreiving the model in case we know the tensor names, or names of collections we are interested in. But how to access important tensors in case we do not know what to look for?\n",
    "\n",
    "For really simple models, simply calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def example_code():\n",
    "    print(tf.get_default_graph().as_graph_def());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "should give us the names and links between tensors. For more complicated graphs, the output becomes too verbose and complicated to understand, so we turn to [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) - a tool for visualizing basically anything within the training process.\n",
    "\n",
    "A bare minimum example of that is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters.\n",
    "model_path = \"./model\"\n",
    "\n",
    "# Load the previous graph.\n",
    "loader = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "\n",
    "# Launch the graph.\n",
    "with tf.Session() as sess:\n",
    "    loader.restore(sess, model_path)  # Restores all the calculated variables, e.g. weights.\n",
    "    \n",
    "    # Write the graph to /tmp/test.\n",
    "    writer = tf.summary.FileWriter(\"/tmp/test\", graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The graph is now saved to folder \"/tmp/test\". To show the graph open up a terminal and run\n",
    "    \n",
    "    tensorboard --logdir=/tmp/test\n",
    "\n",
    "The output should be something like\n",
    "    \n",
    "    Starting TensorBoard 46 at http://0.0.0.0:6006\n",
    "\n",
    "Simply follow the given location from your browser and you should see TensorBoard. Navigate to the GRAPHS section and voil√†! \n",
    "\n",
    "Admittedly, the graph is rather big, and it takes some time to figure out what is happening there, but since we are usually interested in either the cost or final output, we can usually navigate to the end of the graph. In this case we find something like this\n",
    "\n",
    "<img src=\"../images/tensor_graph.png\">\n",
    "\n",
    "We can see both cost and pred tensors are here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
